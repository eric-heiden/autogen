{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to autogen Code generation for automatic differentiation with GPU support. This library leverages CppAD and CppADCodeGen to trace C++ and Python code, and turns it into efficient CUDA or C code. At the same time, the Jacobian and Hessian code can be automatically generated through reverse-mode or forward-mode automatic differentiation. The generated code is compiled to a dynamic library which typically runs orders of magnitude faster than the original user code that was traced, while multiple calls to the forward or backward versions of the function can be parallelized through CUDA or OpenMP.","title":"Welcome to autogen"},{"location":"#welcome-to-autogen","text":"Code generation for automatic differentiation with GPU support. This library leverages CppAD and CppADCodeGen to trace C++ and Python code, and turns it into efficient CUDA or C code. At the same time, the Jacobian and Hessian code can be automatically generated through reverse-mode or forward-mode automatic differentiation. The generated code is compiled to a dynamic library which typically runs orders of magnitude faster than the original user code that was traced, while multiple calls to the forward or backward versions of the function can be parallelized through CUDA or OpenMP.","title":"Welcome to autogen"},{"location":"basics/getting-started/","text":"Getting started Autogen supports numerical code written in C++ or Python to be traced, compiled, and executed in parallel on the CPU and GPU. The following example demonstrates a simple function that computes the norm of a 2D vector: C++ Python #include <autogen/autogen.hpp> template < typename Scalar > // Scalar denotes the number type struct my_function { void operator ()( const std :: vector < Scalar > & input , std :: vector < Scalar > & output ) const { output [ 0 ] = sqrt ( input [ 0 ] * input [ 0 ] + input [ 1 ] * input [ 1 ]); } }; int main ( void ) { std :: vector < double > input , output ( 1 ); input = { 0.5 , 1.5 }; autogen :: Generated < my_function > gen ( \"my_function\" ); gen . set_mode ( autogen :: GENERATE_CUDA ); // the function gets compiled and executed on the GPU gen ( input , output ); return 0 ; } import autogen as ag import numpy as np def my_function ( xs ): return np . sqrt ( xs [ 0 ] ** 2 + xs [ 1 ] ** 2 ) def __main__ (): xs = np . array ([ 0.5 , 1.5 ]) gen = ag . Generated ( \"my_function\" , my_function ) gen . set_mode ( ag . GENERATE_CUDA ) y = gen ( xs ) print ( y )","title":"Getting started"},{"location":"basics/getting-started/#getting-started","text":"Autogen supports numerical code written in C++ or Python to be traced, compiled, and executed in parallel on the CPU and GPU. The following example demonstrates a simple function that computes the norm of a 2D vector: C++ Python #include <autogen/autogen.hpp> template < typename Scalar > // Scalar denotes the number type struct my_function { void operator ()( const std :: vector < Scalar > & input , std :: vector < Scalar > & output ) const { output [ 0 ] = sqrt ( input [ 0 ] * input [ 0 ] + input [ 1 ] * input [ 1 ]); } }; int main ( void ) { std :: vector < double > input , output ( 1 ); input = { 0.5 , 1.5 }; autogen :: Generated < my_function > gen ( \"my_function\" ); gen . set_mode ( autogen :: GENERATE_CUDA ); // the function gets compiled and executed on the GPU gen ( input , output ); return 0 ; } import autogen as ag import numpy as np def my_function ( xs ): return np . sqrt ( xs [ 0 ] ** 2 + xs [ 1 ] ** 2 ) def __main__ (): xs = np . array ([ 0.5 , 1.5 ]) gen = ag . Generated ( \"my_function\" , my_function ) gen . set_mode ( ag . GENERATE_CUDA ) y = gen ( xs ) print ( y )","title":"Getting started"},{"location":"basics/reference/","text":"Reference Generated The Generated object represents the front-end class that supports different modes of computing gradients for a provided functor. The modes are: Numerical (finite differencing), CppAD (forward-/reverse-mode AD tracing), and CodeGen (forward-/reverse-mode AD tracing + code generation for various parallel computation platforms). C++ Python template < template < typename > typename Functor > struct Generated In C++, the function to be differentiated/traced needs to be given as a template parameter for a functor type that accepts the Scalar type as template parameter, and implements the operator() function: void operator ()( const std :: vector < Scalar >& input , std :: vector < Scalar >& output ) const Constructor template < typename ... Args > Generated ( const std :: string & name , Args && ... args ) The constructor takes the name of the functor and the (optional) arguments to be passed to the constructor of the functor. import autogen as ag import numpy as np def my_function ( xs ): return np . sqrt ( xs [ 0 ] ** 2 + xs [ 1 ] ** 2 ) def __main__ (): xs = np . array ([ 0.5 , 1.5 ]) gen = ag . Generated ( \"my_function\" , my_function ) gen . set_mode ( ag . GENERATE_CUDA ) y = gen ( xs ) print ( y )","title":"Reference"},{"location":"basics/reference/#reference","text":"","title":"Reference"},{"location":"basics/reference/#generated","text":"The Generated object represents the front-end class that supports different modes of computing gradients for a provided functor. The modes are: Numerical (finite differencing), CppAD (forward-/reverse-mode AD tracing), and CodeGen (forward-/reverse-mode AD tracing + code generation for various parallel computation platforms). C++ Python template < template < typename > typename Functor > struct Generated In C++, the function to be differentiated/traced needs to be given as a template parameter for a functor type that accepts the Scalar type as template parameter, and implements the operator() function: void operator ()( const std :: vector < Scalar >& input , std :: vector < Scalar >& output ) const","title":"Generated"},{"location":"basics/reference/#constructor","text":"template < typename ... Args > Generated ( const std :: string & name , Args && ... args ) The constructor takes the name of the functor and the (optional) arguments to be passed to the constructor of the functor. import autogen as ag import numpy as np def my_function ( xs ): return np . sqrt ( xs [ 0 ] ** 2 + xs [ 1 ] ** 2 ) def __main__ (): xs = np . array ([ 0.5 , 1.5 ]) gen = ag . Generated ( \"my_function\" , my_function ) gen . set_mode ( ag . GENERATE_CUDA ) y = gen ( xs ) print ( y )","title":"Constructor"},{"location":"basics/vectorization/","text":"Vectorized function calls The Generated class provides vectorized functions for the forward and backward passes of the traced function, which allows to run the function on multiple inputs in parallel. When using the vectorized mode, the input is split into global and local memory. The global memory is shared between the parallel threads, while the local memory is split up into segments for each thread.","title":"Vectorized function calls"},{"location":"basics/vectorization/#vectorized-function-calls","text":"The Generated class provides vectorized functions for the forward and backward passes of the traced function, which allows to run the function on multiple inputs in parallel. When using the vectorized mode, the input is split into global and local memory. The global memory is shared between the parallel threads, while the local memory is split up into segments for each thread.","title":"Vectorized function calls"}]}